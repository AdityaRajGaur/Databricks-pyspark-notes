{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5120f53-12e5-4a7a-95dd-fb3f6b58ee58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82e5fa6d-f316-47d1-a837-975ba7335829",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## This is a practice notebook\n",
    "### Creating dataframes using data and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89ad5b5c-6ba2-4df9-8057-3063cfbade0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-------------+\n|     Name|Age|         City|\n+---------+---+-------------+\n|    Alice| 30|     New York|\n|      Bob| 25|San Francisco|\n|Catherine| 27|      Chicago|\n|    David| 35|       Boston|\n|      Eva| 28|      Seattle|\n+---------+---+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Define the schema\n",
    "\n",
    "from pyspark.sql.types import StringType,StructField,StructType,IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\",StringType(),True),\n",
    "    StructField(\"Age\",StringType(),True),\n",
    "    StructField(\"City\",StringType(),True),\n",
    "                     ])\n",
    "#Create a list of sample data\n",
    "data = [\n",
    "    (\"Alice\",30,\"New York\"),\n",
    "    (\"Bob\", 25, \"San Francisco\"),\n",
    "    (\"Catherine\", 27, \"Chicago\"),\n",
    "    (\"David\", 35, \"Boston\"),\n",
    "    (\"Eva\", 28, \"Seattle\")\n",
    "]\n",
    "\n",
    "# Create DataFrame using the data\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# show data\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b192b1aa-3988-4f4c-84c8-786e7030329c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reading data from a csv file in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d79a39d-d099-4652-9ab7-3cc30c98d857",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-------------+\n|     Name|Age|         City|\n+---------+---+-------------+\n|    Alice| 30|     New York|\n|      Bob| 25|San Francisco|\n|Catherine| 27|      Chicago|\n|    David| 35|       Boston|\n|      Eva| 28|      Seattle|\n+---------+---+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\",StringType(),True),\n",
    "    StructField(\"Age\",IntegerType(),True),\n",
    "    StructField(\"City\",StringType(),True)\n",
    "])\n",
    "\n",
    "#file path dbfs:/FileStore/shared_uploads/rajadityagaur@gmail.com/sample_data.csv\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/rajadityagaur@gmail.com/sample_data.csv\")\n",
    "\n",
    "#show df\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e788a2e-8b78-486a-8001-1307bd758967",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Adding extra data to the dataframe for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e713d9b-1627-4463-a2fe-f9ea62ae3ab3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#additional datta\n",
    "additional_data = [\n",
    "    (\"Frank\", 29, \"Los Angeles\"),\n",
    "    (\"Grace\", 31, \"Houston\")\n",
    "]\n",
    "#creating df for this new data\n",
    "additional_df = spark.createDataFrame(additional_data,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "313be57c-da8d-44dc-9095-2a9ed56579f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+\n| Name|Age|       City|\n+-----+---+-----------+\n|Frank| 29|Los Angeles|\n|Grace| 31|    Houston|\n+-----+---+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "additional_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88bf7751-f610-4f18-9774-b1451eb07b0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### adding both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af445e07-6872-4599-ba00-d3e13fe6081e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-------------+\n|     Name|Age|         City|\n+---------+---+-------------+\n|    Alice| 30|     New York|\n|      Bob| 25|San Francisco|\n|Catherine| 27|      Chicago|\n|    David| 35|       Boston|\n|      Eva| 28|      Seattle|\n|    Frank| 29|  Los Angeles|\n|    Grace| 31|      Houston|\n+---------+---+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "combined_df = df.union(additional_df)\n",
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa3f0866-8c1e-43ed-9984-655abd5d7901",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----------+\n| Name| Age|       City|\n+-----+----+-----------+\n|Frank|  29|Los Angeles|\n|Grace|  31|    Houston|\n|Alice|  30|   New York|\n| null|  22|    Phoenix|\n| Hank|null|     Denver|\n|  990|  25|          0|\n+-----+----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Define additional data including duplicates and null values\n",
    "additional_data = [\n",
    "    (\"Frank\", 29, \"Los Angeles\"),  # Normal data\n",
    "    (\"Grace\", 31, \"Houston\"),      # Normal data\n",
    "    (\"Alice\", 30, \"New York\"),     # Duplicate of existing row\n",
    "    (None, 22, \"Phoenix\"),         # Null value in Name\n",
    "    (\"Hank\", None, \"Denver\"),      # Null value in Age\n",
    "    (990, 25, 0)              # Null value in City\n",
    "]\n",
    "\n",
    "#this will work since \"STRING-TYPE ACCEPTS NUMBERS BUT INTEGER-TYPE DOES NOT ACCEPT STRING.\"\n",
    "test_df = spark.createDataFrame(additional_data,schema)\n",
    "\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edba72bc-7c8f-4e54-a080-7fc6001a5535",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Handling null values and duplicates values and reading data in various formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0f7ce33-3688-40b4-bfaa-1eb725c787ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-------------+\n|     Name| Age|         City|\n+---------+----+-------------+\n|    Alice|  30|     New York|\n|      Bob|  25|San Francisco|\n|Catherine|  27|      Chicago|\n|    David|  35|       Boston|\n|      Eva|  28|      Seattle|\n|    Frank|  29|  Los Angeles|\n|    Grace|  31|      Houston|\n|    Alice|  30|     New York|\n|     null|  22|      Phoenix|\n|     Hank|null|       Denver|\n|      Ivy|  25|         null|\n+---------+----+-------------+\n\nDataframe after dropping rows with any null values: \n+---------+---+-------------+\n|     Name|Age|         City|\n+---------+---+-------------+\n|    Alice| 30|     New York|\n|      Bob| 25|San Francisco|\n|Catherine| 27|      Chicago|\n|    David| 35|       Boston|\n|      Eva| 28|      Seattle|\n|    Frank| 29|  Los Angeles|\n|    Grace| 31|      Houston|\n|    Alice| 30|     New York|\n+---------+---+-------------+\n\n+---------+---+-------------+\n|     Name|Age|         City|\n+---------+---+-------------+\n|    Alice| 30|     New York|\n|      Bob| 25|San Francisco|\n|Catherine| 27|      Chicago|\n|    David| 35|       Boston|\n|      Eva| 28|      Seattle|\n|    Frank| 29|  Los Angeles|\n|    Grace| 31|      Houston|\n|    Alice| 30|     New York|\n|  Unknown| 22|      Phoenix|\n|     Hank|  0|       Denver|\n|      Ivy| 25|      Unknown|\n+---------+---+-------------+\n\nDataFrame after dropping rows with null values in specific columns:\n+---------+----+-------------+\n|     Name| Age|         City|\n+---------+----+-------------+\n|    Alice|  30|     New York|\n|      Bob|  25|San Francisco|\n|Catherine|  27|      Chicago|\n|    David|  35|       Boston|\n|      Eva|  28|      Seattle|\n|    Frank|  29|  Los Angeles|\n|    Grace|  31|      Houston|\n|    Alice|  30|     New York|\n|     Hank|null|       Denver|\n+---------+----+-------------+\n\nDataFrame after removing duplicate rows:\n+---------+----+-------------+\n|     Name| Age|         City|\n+---------+----+-------------+\n|    Alice|  30|     New York|\n|      Bob|  25|San Francisco|\n|    David|  35|       Boston|\n|Catherine|  27|      Chicago|\n|      Eva|  28|      Seattle|\n|    Frank|  29|  Los Angeles|\n|    Grace|  31|      Houston|\n|     null|  22|      Phoenix|\n|     Hank|null|       Denver|\n|      Ivy|  25|         null|\n+---------+----+-------------+\n\nDataFrame after removing duplicate rows based on specific columns:\n+---------+----+-------------+\n|     Name| Age|         City|\n+---------+----+-------------+\n|    Alice|  30|     New York|\n|      Bob|  25|San Francisco|\n|Catherine|  27|      Chicago|\n|    David|  35|       Boston|\n|      Eva|  28|      Seattle|\n|    Frank|  29|  Los Angeles|\n|    Grace|  31|      Houston|\n|     null|  22|      Phoenix|\n|     Hank|null|       Denver|\n|      Ivy|  25|         null|\n+---------+----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField,StructType,IntegerType,StringType\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"Alice\", 30, \"New York\"),\n",
    "    (\"Bob\", 25, \"San Francisco\"),\n",
    "    (\"Catherine\", 27, \"Chicago\"),\n",
    "    (\"David\", 35, \"Boston\"),\n",
    "    (\"Eva\", 28, \"Seattle\"),\n",
    "    (\"Frank\", 29, \"Los Angeles\"),  # Normal data\n",
    "    (\"Grace\", 31, \"Houston\"),      # Normal data\n",
    "    (\"Alice\", 30, \"New York\"),     # Duplicate of existing row\n",
    "    (None, 22, \"Phoenix\"),         # Null value in Name\n",
    "    (\"Hank\", None, \"Denver\"),      # Null value in Age\n",
    "    (\"Ivy\", 25, None)              # Null value in City\n",
    "]\n",
    "schema = StructType([\n",
    "    StructField(\"Name\",StringType(),True),\n",
    "    StructField(\"Age\",IntegerType(),True),\n",
    "    StructField(\"City\",StringType(),True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "#print original Df\n",
    "df.show()\n",
    "\n",
    "# Handling null values - deleting all rows having any null value\n",
    "print(\"Dataframe after dropping rows with any null values: \")\n",
    "df_no_nulls = df.dropna()\n",
    "df_no_nulls.show()\n",
    "\n",
    "# Filling null values\n",
    "df_filled = df.fillna({\n",
    "    \"Name\":\"Unknown\",\n",
    "    \"Age\":0,\n",
    "    \"City\":\"Unknown\"\n",
    "})\n",
    "df_filled.show()\n",
    "\n",
    "#Dropping rows where specific column is null\n",
    "print(\"DataFrame after dropping rows with null values in specific columns:\")\n",
    "df_no_nulls_subset = df.dropna(subset = [\"Name\",\"City\"])\n",
    "df_no_nulls_subset.show()\n",
    "\n",
    "# Handling duplicates - removing duplicate rows\n",
    "print(\"DataFrame after removing duplicate rows:\")\n",
    "df_no_duplicates = df.dropDuplicates()\n",
    "df_no_duplicates.show()\n",
    "\n",
    "print(\"DataFrame after removing duplicate rows based on specific columns:\")\n",
    "df_no_duplicates_subset = df.dropDuplicates(subset = [\"Name\",\"City\"])\n",
    "\n",
    "df_no_duplicates_subset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbab8bc9-5387-41fb-ac57-acb93f6c45b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00039809-c09c-4ba0-b3f8-b506139ad39d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----------+\n|     Name|Age|       City|\n+---------+---+-----------+\n|    Alice| 30|   New York|\n|Catherine| 27|    Chicago|\n|    David| 35|     Boston|\n|      Eva| 28|    Seattle|\n|    Frank| 29|Los Angeles|\n|    Grace| 31|    Houston|\n|    Alice| 30|   New York|\n+---------+---+-----------+\n\n+---------+---+-----------+\n|     Name|Age|       City|\n+---------+---+-----------+\n|    Alice| 30|   New York|\n|Catherine| 27|    Chicago|\n|    David| 35|     Boston|\n|      Eva| 28|    Seattle|\n|    Frank| 29|Los Angeles|\n|    Grace| 31|    Houston|\n|    Alice| 30|   New York|\n+---------+---+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df.filter(df['Age'] > 25)\n",
    "df_filtered.show()\n",
    "\n",
    "#sql like syntax\n",
    "df_filtered = df.filter(\"Age > 25\")\n",
    "df_filtered.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db259f07-65da-4ff4-a1fa-294429bb2268",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## sorting data by single and multiple column\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d6ca42-eea5-4a70-b9d4-3892c4157e0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-------------+\n|     Name| Age|         City|\n+---------+----+-------------+\n|     Hank|null|       Denver|\n|     null|  22|      Phoenix|\n|      Bob|  25|San Francisco|\n|      Ivy|  25|         null|\n|Catherine|  27|      Chicago|\n|      Eva|  28|      Seattle|\n|    Frank|  29|  Los Angeles|\n|    Alice|  30|     New York|\n|    Alice|  30|     New York|\n|    Grace|  31|      Houston|\n|    David|  35|       Boston|\n+---------+----+-------------+\n\n+---------+----+-------------+\n|     Name| Age|         City|\n+---------+----+-------------+\n|     Hank|null|       Denver|\n|     null|  22|      Phoenix|\n|      Ivy|  25|         null|\n|      Bob|  25|San Francisco|\n|Catherine|  27|      Chicago|\n|      Eva|  28|      Seattle|\n|    Frank|  29|  Los Angeles|\n|    Alice|  30|     New York|\n|    Alice|  30|     New York|\n|    Grace|  31|      Houston|\n|    David|  35|       Boston|\n+---------+----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#sorting by a single column\n",
    "df_sorted_single_col = df.sort(\"Age\")\n",
    "df_sorted_single_col.show()\n",
    "\n",
    "#Multiple columns\n",
    "df_sorted_multiple_col = df.sort(df[\"Age\"].asc(),df[\"Name\"].desc())\n",
    "df_sorted_multiple_col.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fe8f987-a48a-47a0-b4a0-1e7fbad7c06c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a4fe1e9-4de9-4012-8144-a0e752037935",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------+--------+\n|         City|AverageAge|MaxAge|MinAge|TotalAge|\n+-------------+----------+------+------+--------+\n|     New York|      30.0|    30|    30|      60|\n|San Francisco|      25.0|    25|    25|      25|\n|      Chicago|      27.0|    27|    27|      27|\n|       Boston|      35.0|    35|    35|      35|\n|      Seattle|      28.0|    28|    28|      28|\n|  Los Angeles|      29.0|    29|    29|      29|\n|      Houston|      31.0|    31|    31|      31|\n|      Phoenix|      22.0|    22|    22|      22|\n|         null|      25.0|    25|    25|      25|\n|       Denver|      null|  null|  null|    null|\n+-------------+----------+------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min,max,avg,sum\n",
    "\n",
    "df_agg = df.groupBy(\"City\").agg(\n",
    "    avg(\"Age\").alias(\"AverageAge\"),\n",
    "    max(\"Age\").alias(\"MaxAge\"),\n",
    "    min(\"Age\").alias(\"MinAge\"),\n",
    "    sum(\"Age\").alias(\"TotalAge\")\n",
    ")\n",
    "\n",
    "df_agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "153a686d-9a9e-4847-a494-72f69f488681",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53479f6d-aeda-42d3-97a5-d0d3ba95cbfd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-------------+----------+----+----------+\n|     Name| Age|         City|row_number|rank|dense_rank|\n+---------+----+-------------+----------+----+----------+\n|      Ivy|  25|         null|         1|   1|         1|\n|    David|  35|       Boston|         1|   1|         1|\n|Catherine|  27|      Chicago|         1|   1|         1|\n|     Hank|null|       Denver|         1|   1|         1|\n|    Grace|  31|      Houston|         1|   1|         1|\n|    Frank|  29|  Los Angeles|         1|   1|         1|\n|    Alice|  30|     New York|         1|   1|         1|\n|    Alice|  30|     New York|         2|   1|         1|\n|     null|  22|      Phoenix|         1|   1|         1|\n|      Bob|  25|San Francisco|         1|   1|         1|\n|      Eva|  28|      Seattle|         1|   1|         1|\n+---------+----+-------------+----------+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank\n",
    "\n",
    "window_spec = Window.partitionBy(\"City\").orderBy(\"Age\")\n",
    "\n",
    "df_window = df.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "df_window = df_window.withColumn(\"rank\", rank().over(window_spec))\n",
    "df_window = df_window.withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
    "\n",
    "df_window.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f1cf5cd-ac95-4ad2-a4b1-f058ac67519c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Working with Time Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6e24ff5-0092-4d17-aa18-e6ea7f0573e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Date and Timestamp Conversion:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6677daa9-c5ef-406f-8127-d0aef1522528",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Extracting Parts of Date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e80041d9-ac04-4cbf-8776-e67aeae7001b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e9b9db6-4650-47c7-bf3e-8a692c4498d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca60350b-ce81-4032-86f3-3ebd6bd8c058",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### GroupBy and Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e36daed-1f2c-468f-b4c5-8fb52d69d625",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+\n|         City|TotalAge|AverageAge|\n+-------------+--------+----------+\n|     New York|      60|      30.0|\n|San Francisco|      25|      25.0|\n|      Chicago|      27|      27.0|\n|       Boston|      35|      35.0|\n|      Seattle|      28|      28.0|\n|  Los Angeles|      29|      29.0|\n|      Houston|      31|      31.0|\n|      Phoenix|      22|      22.0|\n|         null|      25|      25.0|\n|       Denver|    null|      null|\n+-------------+--------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_grouped = df.groupBy(\"City\").agg(\n",
    "    sum(\"Age\").alias(\"TotalAge\"),\n",
    "    avg(\"Age\").alias(\"AverageAge\")\n",
    ")\n",
    "df_grouped.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13950961-984e-4e86-b2ec-4c5e1e50c8f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## UDFs (User-Defined Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa26242b-159d-4ea5-b858-ca9d7450e084",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPythonException\u001B[0m                           Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1721858934126211>:9\u001B[0m\n",
       "\u001B[1;32m      7\u001B[0m add_one_udf \u001B[38;5;241m=\u001B[39m udf(add_one, IntegerType())\n",
       "\u001B[1;32m      8\u001B[0m df_udf \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAgePlusOne\u001B[39m\u001B[38;5;124m\"\u001B[39m, add_one_udf(df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAge\u001B[39m\u001B[38;5;124m\"\u001B[39m]))\n",
       "\u001B[0;32m----> 9\u001B[0m df_udf\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n",
       "\u001B[1;32m    914\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m    915\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_BOOLEAN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    916\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m    917\u001B[0m     )\n",
       "\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n",
       "\u001B[0;32m--> 920\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m    921\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    922\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mPythonException\u001B[0m: \n",
       "  An exception was thrown from the Python worker. Please see the stack trace below.\n",
       "'TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'', from <command-1721858934126211>, line 5. Full traceback below:\n",
       "Traceback (most recent call last):\n",
       "  File \"<command-1721858934126211>\", line 5, in add_one\n",
       "TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPythonException\u001B[0m                           Traceback (most recent call last)\nFile \u001B[0;32m<command-1721858934126211>:9\u001B[0m\n\u001B[1;32m      7\u001B[0m add_one_udf \u001B[38;5;241m=\u001B[39m udf(add_one, IntegerType())\n\u001B[1;32m      8\u001B[0m df_udf \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAgePlusOne\u001B[39m\u001B[38;5;124m\"\u001B[39m, add_one_udf(df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAge\u001B[39m\u001B[38;5;124m\"\u001B[39m]))\n\u001B[0;32m----> 9\u001B[0m df_udf\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    914\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m    915\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_BOOLEAN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    916\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m    917\u001B[0m     )\n\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n\u001B[0;32m--> 920\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    921\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    922\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mPythonException\u001B[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\n'TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'', from <command-1721858934126211>, line 5. Full traceback below:\nTraceback (most recent call last):\n  File \"<command-1721858934126211>\", line 5, in add_one\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n",
       "errorSummary": "<span class='ansi-red-fg'>PythonException</span>: \n  An exception was thrown from the Python worker. Please see the stack trace below.\n'TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'', from <command-1721858934126211>, line 5. Full traceback below:\nTraceback (most recent call last):\n  File \"<command-1721858934126211>\", line 5, in add_one\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c6c6dbd-308f-4465-93e9-8498db8e7e9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read and Write CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e72304a-cb4b-4064-9a1c-b52cbe73cba7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reading\n",
    "# df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/path/to/file.csv\")\n",
    "# df = spark.read.format(\"json\").load(\"/path/to/file.json\")\n",
    "# df = spark.read.format(\"parquet\").load(\"/path/to/file.parquet\")\n",
    "\n",
    "#writing\n",
    "# df.write.format(\"csv\").option(\"header\", \"true\").save(\"/path/to/save\")\n",
    "# df.write.format(\"json\").save(\"/path/to/save\")\n",
    "# df.write.format(\"parquet\").save(\"/path/to/save\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4a97ea0-e673-426d-a1b6-30c65b9c175f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Running sql queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7bf0317-5fa0-4a35-9b02-25fd1a0d4cef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----------+\n|     Name|Age|       City|\n+---------+---+-----------+\n|    Alice| 30|   New York|\n|Catherine| 27|    Chicago|\n|    David| 35|     Boston|\n|      Eva| 28|    Seattle|\n|    Frank| 29|Los Angeles|\n|    Grace| 31|    Houston|\n|    Alice| 30|   New York|\n+---------+---+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"people\")\n",
    "sql_df = spark.sql(\"SELECT * FROM people WHERE AGE > 25\")\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83f9ac19-286d-4508-97f2-c7e82a8b42b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2024-05-17 14:59:28",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
